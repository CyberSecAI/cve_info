{
    "cveId": "CVE-2025-49847",
    "version": "1.0.0",
    "timestamp": "2025-07-04T16:12:41.853043+00:00",
    "description": "llama.cpp is an inference of several LLM models in C/C++. Prior to version b5662, an attackersupplied GGUF model vocabulary can trigger a buffer overflow in llama.cpps vocabularyloading code. Specifically, the helper _try_copy in llama.cpp/src/vocab.cpp llama_vocabimpltoken_to_piece() casts a very large size_t token length into an int32_t, causing the length check (if (length < (int32_t)size)) to be bypassed. As a result, memcpy is still called with that oversized size, letting a malicious model overwrite memory beyond the intended buffer. This can lead to arbitrary memory corruption and potential code execution. This issue has been patched in version b5662.",
    "keyphrases": {
        "component": "vocabularyloading code",
        "rootcause": "",
        "vector": "",
        "weakness": "buffer overflow, memory corruption",
        "product": "llama.cpp",
        "impact": "arbitrary memory corruption, code execution",
        "attacker": "attackersupplied GGUF model vocabulary",
        "version": "prior to b5662"
    }
}
