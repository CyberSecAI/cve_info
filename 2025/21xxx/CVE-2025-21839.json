{
    "cveId": "CVE-2025-21839",
    "version": "1.0.0",
    "timestamp": "2025-07-04T16:12:41.853043+00:00",
    "description": "In the Linux kernel, the following vulnerability has been resolved KVM x86 Load DR6 with guest value only before entering .vcpu_run() loop Move the conditional loading of hardware DR6 with the guests DR6 value out of the core .vcpu_run() loop to fix a bug where KVM can load hardware with a stale vcpu->arch.dr6. When the guest accesses a DR and host userspace isnt debugging the guest, KVM disables DR interception and loads the guests values into hardware on VM-Enter and saves them on VM-Exit. This allows the guest to access DRs at will, e.g. so that a sequence of DR accesses to configure a breakpoint only generates one VM-Exit. For DR0-DR3, the logic/behavior is identical between VMX and SVM, and also identical between KVM_DEBUGREG_BP_ENABLED (userspace debugging the guest) and KVM_DEBUGREG_WONT_EXIT (guest using DRs), and so KVM handles loading DR0-DR3 in common code, _outside_ of the core kvm_x86_ops.vcpu_run() loop. But for DR6, the guests value doesnt need to be loaded into hardware for KVM_DEBUGREG_BP_ENABLED, and SVM provides a dedicated VMCB field whereas VMX requires software to manually load the guest value, and so loading the guests value into DR6 is handled by {svm,vmx}_vcpu_run(), i.e. is done _inside_ the core run loop. Unfortunately, saving the guest values on VM-Exit is initiated by common x86, again outside of the core run loop. If the guest modifies DR6 (in hardware, when DR interception is disabled), and then the next VM-Exit is a fastpath VM-Exit, KVM will reload hardware DR6 with vcpu->arch.dr6 and clobber the guests actual value. The bug shows up primarily with nested VMX because KVM handles the VMX preemption timer in the fastpath, and the window between hardware DR6 being modified (in guest context) and DR6 being read by guest software is orders of magnitude larger in a nested setup. E.g. in non-nested, the VMX preemption timer would need to fire precisely between #DB injection and the #DB handlers read of DR6, whereas with a KVM-on-KVM setup, the window where hardware DR6 is dirty extends all the way from L1 writing DR6 to VMRESUME (in L1). L1s view ========== CPU 0/KVM-7289 [023] d.... 2925.640961 kvm_entry vcpu 0 A L1 Writes DR6 CPU 0/KVM-7289 [023] d.... 2925.640963 Set DRs, DR6 = 0xffff0ff1 B CPU 0/KVM-7289 [023] d.... 2925.640967 kvm_exit vcpu 0 reason EXTERNAL_INTERRUPT intr_info 0x800000ec D L1 reads DR6, arch.dr6 = 0 CPU 0/KVM-7289 [023] d.... 2925.640969 Sync DRs, DR6 = 0xffff0ff0 CPU 0/KVM-7289 [023] d.... 2925.640976 kvm_entry vcpu 0 L2 reads DR6, L1 disables DR interception CPU 0/KVM-7289 [023] d.... 2925.640980 kvm_exit vcpu 0 reason DR_ACCESS info1 0x0000000000000216 CPU 0/KVM-7289 [023] d.... 2925.640983 kvm_entry vcpu 0 CPU 0/KVM-7289 [023] d.... 2925.640983 Set DRs, DR6 = 0xffff0ff0 L2 detects failure CPU 0/KVM-7289 [023] d.... 2925.640987 kvm_exit vcpu 0 reason HLT L1 reads DR6 (confirms failure) CPU 0/KVM-7289 [023] d.... 2925.640990 Sync DRs, DR6 = 0xffff0ff0 L0s view ========== L2 reads DR6, arch.dr6 = 0 CPU 23/KVM-5046 [001] d.... 3410.005610 kvm_exit vcpu 23 reason DR_ACCESS info1 0x0000000000000216 CPU 23/KVM-5046 [001] ..... 3410.005610 kvm_nested_vmexit vcpu 23 reason DR_ACCESS info1 0x0000000000000216 L2 => L1 nested VM-Exit CPU 23/KVM-5046 [001] ..... 3410.005610 kvm_nested_vmexit_inject reason DR_ACCESS ext_inf1 0x0000000000000216 CPU 23/KVM-5046 [001] d.... 3410.005610 kvm_entry vcpu 23 CPU 23/KVM-5046 [001] d.... 3410.005611 kvm_exit vcpu 23 reason VMREAD CPU 23/KVM-5046 [001] d.... 3410.005611 kvm_entry vcpu 23 CPU 23/KVM-5046 [001] d.... 3410. ---truncated---",
    "keyphrases": {
        "rootcause": "",
        "weakness": "",
        "impact": "clobber guests actual value",
        "vector": "",
        "attacker": "",
        "product": "Linux kernel",
        "version": "",
        "component": ""
    }
}
