{
    "cveId": "CVE-2024-12704",
    "version": "1.0.0",
    "timestamp": "2025-07-04T16:12:41.853043+00:00",
    "description": "A vulnerability in the LangChainLLM class of the run-llama/llama_index repository, version v0.12.5, allows for a Denial of Service (DoS) attack. The stream_complete method executes the llm using a thread and retrieves the result via the get_response_gen method of the StreamingGeneratorCallbackHandler class. If the thread terminates abnormally before the _llm.predict is executed, there is no exception handling for this case, leading to an infinite loop in the get_response_gen function. This can be triggered by providing an input of an incorrect type, causing the thread to terminate and the process to continue running indefinitely.",
    "keyphrases": {
        "component": "stream_complete method",
        "rootcause": "['thread termination before _llm.predict', 'no exception handling']",
        "vector": "incorrect input type",
        "weakness": "infinite loop",
        "product": "LangChainLLM class of run-llama/llama_index",
        "impact": "Denial of Service (DoS)",
        "attacker": "",
        "version": "v0.12.5"
    }
}
